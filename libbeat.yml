################### Cloudflarebeat Configuration Example #########################

############################# Cloudflarebeat ######################################

cloudflarebeat:
  #### Cloudflare Common Settings ####
  # These settings can be applied on cloudflarebeat level, and/or
  # on consumer level. This way you can provide defaults for all
  # zones, with the option to override them for specific zones.

  # Credentials
  api_email: "user@example.com"
  api_key: "0123456789abcdef"

  # Maximum allow time period to request logs for in a single request.
  # This setting should be lowered for zones with very high traffic volume.
  # That's due to a limitation with Cloudflare's ELS system.
  # See the "Requests will fail when" section at
  # https://support.cloudflare.com/hc/en-us/articles/216672448-Enterprise-Log-Share-Logpull-REST-API
  #max_pull_size: 60m

  # Limit how far back we start importing logs.
  # This is only relevant for the initial import, when starting this beat for
  # the first time, or when stopping this beat. When it is stopped
  # longer than this priod of time, you'll "lose" logs.
  #oldest_allowed: 120m


  ## The following settings shouldn't be changed from their default

  # It takes some time for logs to become available in Cloudflare's system.
  # The default will limit us to logs older than 10 minutes.
  #newest_allowed: 10m

  # The main log importer loop won't request logs if the time range to be
  # imported is ness than 1 minute.
  #margin: 1m

  # The amount of time to wait before retying in case an error occurrs.
  #backoff: 30s

  # New logs will be requested ever 5 minutes.
  #pull_interval: 5m

  # Distributes log pull evenly within pull_interval
  # This is meant to reduce stress on the output.
  #pull_offset_enabled: true


  #### Clouflare Log Consumers ####

  # Define for which zones logs should be imported.
  # The minimum valid config looks like this:
  #consumers:
  #  - zones:
  #      - example.com

  consumers:
    # small websites
    - oldest_allowed: 24h
      zones:
        - example.com
        - example.com

    # medium traffic volume websites; shorter max_pull_size, see above
    - max_pull_size: 30m
      zones:
        - example.com

    # very high traffic volume websites; very short max_pull_size, see above
    - max_pull_size: 5m
      zones:
        - example.com

  # List of fields you want to be logged
  # If necessary, fields can also be specified (or overridden)
  # on consumer level.
  fields:
    - "CacheCacheStatus"
    - "CacheResponseBytes"
    - "CacheResponseStatus"
    - "CacheTieredFill"
    - "ClientASN"
    - "ClientCountry"
    - "ClientDeviceType"
    - "ClientIP"
    - "ClientIPClass"
    - "ClientRequestBytes"
    - "ClientRequestHost"
    - "ClientRequestMethod"
    - "ClientRequestProtocol"
    - "ClientRequestReferer"
    - "ClientRequestURI"
    - "ClientRequestUserAgent"
    - "ClientSSLCipher"
    - "ClientSSLProtocol"
    - "ClientSrcPort"
    - "EdgeColoID"
    - "EdgeEndTimestamp"
    - "EdgePathingOp"
    - "EdgePathingSrc"
    - "EdgePathingStatus"
    - "EdgeRequestHost"
    - "EdgeResponseBytes"
    - "EdgeResponseCompressionRatio"
    - "EdgeResponseContentType"
    - "EdgeResponseStatus"
    - "EdgeServerIP"
    - "EdgeStartTimestamp"
    - "OriginIP"
    - "OriginResponseBytes"
    - "OriginResponseHTTPExpires"
    - "OriginResponseHTTPLastModified"
    - "OriginResponseStatus"
    - "OriginResponseTime"
    - "OriginSSLProtocol"
    - "RayID"
    - "SecurityLevel"
    - "WAFAction"
    - "WAFFlags"
    - "WAFMatchedVar"
    - "WAFProfile"
    - "WAFRuleID"
    - "WAFRuleMessage"
    - "ZoneID"

#================================ General =====================================

# The name of the shipper that publishes the network data. It can be used to group
# all the transactions sent by a single shipper in the web interface.
#name:

# The tags of the shipper are included in their own field with each
# transaction published.
#tags: ["service-X", "web-tier"]

# Optional fields that you can specify to add additional information to the
# output.
#fields:
#  env: staging


#============================== Dashboards =====================================
# These settings control loading the sample dashboards to the Kibana index. Loading
# the dashboards is disabled by default and can be enabled either by setting the
# options here or by using the `setup` command.
#setup.dashboards.enabled: false

# The URL from where to download the dashboards archive. By default this URL
# has a value which is computed based on the Beat name and version. For released
# versions, this URL points to the dashboard archive on the artifacts.elastic.co
# website.
#setup.dashboards.url:

#============================== Kibana =====================================

# Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API.
# This requires a Kibana endpoint configuration.
setup.kibana:

  # Kibana Host
  # Scheme and port can be left out and will be set to the default (http and 5601)
  # In case you specify and additional path, the scheme is required: http://localhost:5601/path
  # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601
  #host: "localhost:5601"

  # Kibana Space ID
  # ID of the Kibana Space into which the dashboards should be loaded. By default,
  # the Default Space will be used.
  #space.id:

#============================= Elastic Cloud ==================================

# These settings simplify using libbeat with the Elastic Cloud (https://cloud.elastic.co/).

# The cloud.id setting overwrites the `output.elasticsearch.hosts` and
# `setup.kibana.host` options.
# You can find the `cloud.id` in the Elastic Cloud web UI.
#cloud.id:

# The cloud.auth setting overwrites the `output.elasticsearch.username` and
# `output.elasticsearch.password` settings. The format is `<user>:<pass>`.
#cloud.auth:

#================================ Outputs =====================================

# Configure what output to use when sending the data collected by the beat.

#-------------------------- Elasticsearch output ------------------------------
output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["localhost:9200"]

  # Optional protocol and basic auth credentials.
  #protocol: "https"
  #username: "elastic"
  #password: "changeme"

#----------------------------- Logstash output --------------------------------
#output.logstash:
  # The Logstash hosts
  #hosts: ["localhost:5044"]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"

#================================ Processors =====================================

# Configure processors to enhance or manipulate events generated by the beat.

processors:
  - add_host_metadata: ~
  - add_cloud_metadata: ~

#================================ Logging =====================================

# Sets log level. The default log level is info.
# Available log levels are: error, warning, info, debug
#logging.level: debug

# At debug level, you can selectively enable logging only for some components.
# To enable all selectors use ["*"]. Examples of other selectors are "beat",
# "publish", "service".
#logging.selectors: ["*"]

#============================== Xpack Monitoring ===============================
# libbeat can export internal metrics to a central Elasticsearch monitoring
# cluster.  This requires xpack monitoring to be enabled in Elasticsearch.  The
# reporting is disabled by default.

# Set to true to enable the monitoring reporter.
#monitoring.enabled: false

# Uncomment to send the metrics to Elasticsearch. Most settings from the
# Elasticsearch output are accepted here as well.
# Note that the settings should point to your Elasticsearch *monitoring* cluster.
# Any setting that is not set is automatically inherited from the Elasticsearch
# output configuration, so if you have the Elasticsearch output configured such
# that it is pointing to your Elasticsearch monitoring cluster, you can simply
# uncomment the following line.
#monitoring.elasticsearch:

#================================= Migration ==================================

# This allows to enable 6.7 migration aliases
#migration.6_to_7.enabled: true
